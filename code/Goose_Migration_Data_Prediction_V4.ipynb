{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Goose Migration Data Prediction V4",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSRist0028/animalmigration/blob/main/code/Goose_Migration_Data_Prediction_V4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew8IxpzYFguF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a9b585-05f6-4844-d25d-81a545c9fe79"
      },
      "source": [
        "!pip install geopandas\n",
        "!pip install shapely\n",
        "!pip install meteostat "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: fiona>=1.8 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.8.19)\n",
            "Requirement already satisfied: pyproj>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (3.0.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.1.5)\n",
            "Requirement already satisfied: shapely>=1.6 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.7.1)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (20.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.15.0)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (7.1.2)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (0.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (2018.9)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.7/dist-packages (1.7.1)\n",
            "Requirement already satisfied: meteostat in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from meteostat) (1.19.5)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.7/dist-packages (from meteostat) (1.1.5)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from meteostat) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1->meteostat) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1->meteostat) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dA5jdXkWLxO"
      },
      "source": [
        "import pandas as pd\n",
        "import sklearn.preprocessing\n",
        "import sklearn.manifold \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "import meteostat\n",
        "from meteostat import Stations, Daily\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import datetime as dt\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEoacm-CyUH_"
      },
      "source": [
        "**Change the csv file and whether or not to include temperature here:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paYMy5ZddIYn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "e9856a37-a5c6-4d11-8801-eb3e0603ec22"
      },
      "source": [
        "# import data with combined temperature and tracking information\n",
        "\n",
        "data_raw = pd.read_csv('https://github.com/JSRist0028/animalmigration/blob/0175410b123d51c6463a7d5e1130bdf68373d68c/data/barnacle_geese_data_matrix?raw=true')\n",
        "\n",
        "# geese data: ('https://github.com/JSRist0028/animalmigration/blob/main/data/barnacle_geese_data_matrix?raw=true')\n",
        "# whale data: ('https://raw.githubusercontent.com/JSRist0028/animalmigration/main/data/Azores%20Great%20Whales%20Satellite%20Telemetry%20Program%20.csv')\n",
        "\n",
        "\n",
        "includeTemp =True\n",
        "data_raw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row number</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>animalID</th>\n",
              "      <th>tavg</th>\n",
              "      <th>tmin</th>\n",
              "      <th>tmax</th>\n",
              "      <th>prcp</th>\n",
              "      <th>snow</th>\n",
              "      <th>wdir</th>\n",
              "      <th>wspd</th>\n",
              "      <th>wpgt</th>\n",
              "      <th>pres</th>\n",
              "      <th>tsun</th>\n",
              "      <th>station_lat</th>\n",
              "      <th>station_long</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>7/17/08 0:08</td>\n",
              "      <td>58.561</td>\n",
              "      <td>-156.854</td>\n",
              "      <td>TUSW_KS7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>7/20/08 2:04</td>\n",
              "      <td>58.574</td>\n",
              "      <td>-156.762</td>\n",
              "      <td>TUSW_KS7</td>\n",
              "      <td>8.9</td>\n",
              "      <td>6.7</td>\n",
              "      <td>11.1</td>\n",
              "      <td>9.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>217.9</td>\n",
              "      <td>19.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>58.6833</td>\n",
              "      <td>-156.6500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>7/20/08 2:18</td>\n",
              "      <td>58.564</td>\n",
              "      <td>-156.812</td>\n",
              "      <td>TUSW_KS7</td>\n",
              "      <td>8.9</td>\n",
              "      <td>6.7</td>\n",
              "      <td>11.1</td>\n",
              "      <td>9.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>217.9</td>\n",
              "      <td>19.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>58.6833</td>\n",
              "      <td>-156.6500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>7/20/08 3:46</td>\n",
              "      <td>58.566</td>\n",
              "      <td>-156.834</td>\n",
              "      <td>TUSW_KS7</td>\n",
              "      <td>8.9</td>\n",
              "      <td>6.7</td>\n",
              "      <td>11.1</td>\n",
              "      <td>9.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>217.9</td>\n",
              "      <td>19.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>58.6833</td>\n",
              "      <td>-156.6500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>7/20/08 3:58</td>\n",
              "      <td>58.531</td>\n",
              "      <td>-156.835</td>\n",
              "      <td>TUSW_KS7</td>\n",
              "      <td>8.9</td>\n",
              "      <td>6.7</td>\n",
              "      <td>11.1</td>\n",
              "      <td>9.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>217.9</td>\n",
              "      <td>19.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>58.6833</td>\n",
              "      <td>-156.6500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90372</th>\n",
              "      <td>90373</td>\n",
              "      <td>1/13/11 2:51</td>\n",
              "      <td>38.188</td>\n",
              "      <td>-76.917</td>\n",
              "      <td>TUSW_NS3</td>\n",
              "      <td>-1.6</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>37.8596</td>\n",
              "      <td>-76.8941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90373</th>\n",
              "      <td>90374</td>\n",
              "      <td>1/28/11 7:16</td>\n",
              "      <td>38.104</td>\n",
              "      <td>-76.997</td>\n",
              "      <td>TUSW_NS3</td>\n",
              "      <td>2.1</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>37.8596</td>\n",
              "      <td>-76.8941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90374</th>\n",
              "      <td>90375</td>\n",
              "      <td>1/28/11 9:15</td>\n",
              "      <td>38.152</td>\n",
              "      <td>-76.877</td>\n",
              "      <td>TUSW_NS3</td>\n",
              "      <td>2.1</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>37.8596</td>\n",
              "      <td>-76.8941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90375</th>\n",
              "      <td>90376</td>\n",
              "      <td>2/1/11 10:57</td>\n",
              "      <td>38.102</td>\n",
              "      <td>-77.023</td>\n",
              "      <td>TUSW_NS3</td>\n",
              "      <td>12.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>37.8596</td>\n",
              "      <td>-76.8941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90376</th>\n",
              "      <td>90377</td>\n",
              "      <td>2/5/11 9:19</td>\n",
              "      <td>38.072</td>\n",
              "      <td>-76.920</td>\n",
              "      <td>TUSW_NS3</td>\n",
              "      <td>5.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>37.8596</td>\n",
              "      <td>-76.8941</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90377 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       row number     timestamp  latitude  ...  tsun station_lat  station_long\n",
              "0               1  7/17/08 0:08    58.561  ...   NaN         NaN           NaN\n",
              "1               2  7/20/08 2:04    58.574  ...   NaN     58.6833     -156.6500\n",
              "2               3  7/20/08 2:18    58.564  ...   NaN     58.6833     -156.6500\n",
              "3               4  7/20/08 3:46    58.566  ...   NaN     58.6833     -156.6500\n",
              "4               5  7/20/08 3:58    58.531  ...   NaN     58.6833     -156.6500\n",
              "...           ...           ...       ...  ...   ...         ...           ...\n",
              "90372       90373  1/13/11 2:51    38.188  ...   NaN     37.8596      -76.8941\n",
              "90373       90374  1/28/11 7:16    38.104  ...   NaN     37.8596      -76.8941\n",
              "90374       90375  1/28/11 9:15    38.152  ...   NaN     37.8596      -76.8941\n",
              "90375       90376  2/1/11 10:57    38.102  ...   NaN     37.8596      -76.8941\n",
              "90376       90377   2/5/11 9:19    38.072  ...   NaN     37.8596      -76.8941\n",
              "\n",
              "[90377 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1MhL2YDI2HT"
      },
      "source": [
        "# creates dataframe with only unique dates\n",
        "\n",
        "def date_to_nth_day(date):#, format='%Y%m%d'):\n",
        "    date = pd.to_datetime(date)#, format=format)\n",
        "    new_year_day = pd.Timestamp(year=date.year, month=1, day=1)\n",
        "    return (date - new_year_day).days + 1\n",
        "# source: https://codereview.stackexchange.com/questions/154140/interpret-yyyymmdd-as-the-nth-day-of-the-year\n",
        "\n",
        "\n",
        "# create a new DataFrame with only location data from unique days\n",
        "# this is now redundant\n",
        "data = pd.DataFrame(columns=data_raw.columns) # create header\n",
        "for i, row in data_raw.iterrows():\n",
        "  if i>0: # skip first row\n",
        "    if (date_to_nth_day(row['timestamp']) != date_to_nth_day(data_raw['timestamp'][i-1])): \n",
        "      data = data.append(row, ignore_index=True)\n",
        "      #print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruzq8gYuIK6X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "8e1fe7a9-7e2e-4b84-c319-c41a4a0d2d61"
      },
      "source": [
        "# Splits data into features (today) and targets (tomorrow)\n",
        "def get_todaytomorrow(data, includeTemp):\n",
        "  num_rows = len(data)\n",
        "  today_timestamp = np.empty((0,1))\n",
        "  tomorrow_timestamp = np.empty((0,1))\n",
        "  if includeTemp:\n",
        "    today = np.empty((0,7), dtype='float')#, 'int') #[]\n",
        "  else:\n",
        "    today = np.empty((0,4), 'int') #[]\n",
        "  tomorrow = np.empty((0,3), dtype='float') #, 'int') #[]\n",
        "  for i, row in data.iterrows():\n",
        "    if (i<(num_rows-3)):\n",
        "      if (pd.notnull(row['tavg']) & pd.notnull(data['tavg'][i+1])): # only add pair if both contain basic weather data (avg temp)\n",
        "        if (data['birdID'][i]==data['birdID'][i+1]):                # only add if birdIDs match\n",
        "          today_day = date_to_nth_day(row['timestamp'])\n",
        "          tomorrow_day = date_to_nth_day(data['timestamp'][i+1])\n",
        "#          if (tomorrow_day-today_day == 1):                         # makes sure that the days are consecutive -> already done ^\n",
        "          birdID_today = row['birdID'].replace('a','1')\n",
        "          birdID_today = birdID_today.replace('b', '2')\n",
        "          birdID_tomorrow = data['birdID'][i+1].replace('a','1')\n",
        "          birdID_tomorrow = birdID_tomorrow.replace('b','2')\n",
        "          today_timestamp = np.append(today_timestamp, row['timestamp'])\n",
        "          tomorrow_timestamp = np.append(tomorrow_timestamp, data['timestamp'][i+1])\n",
        "          if includeTemp:\n",
        "            today_entry = np.asarray([int(birdID_today), today_day, row['latitude'], row['longitude'], row['tavg'], row['tmin'], row['tmax'] ])\n",
        "            tomorrow_entry = np.asarray([int(birdID_tomorrow), data['latitude'][i+1], data['longitude'][i+1]]) #no change\n",
        "          else:\n",
        "            today_entry = np.asarray([int(birdID_today), today_day, row['latitude'], row['longitude']])\n",
        "            tomorrow_entry = np.asarray([int(birdID_tomorrow), data['latitude'][i+1], data['longitude'][i+1]]) # no change\n",
        "          today_entry[np.isnan(today_entry)]=0\n",
        "          tomorrow_entry[np.isnan(tomorrow_entry)]=0\n",
        "          today = np.vstack((today,today_entry))\n",
        "          tomorrow = np.vstack((tomorrow, tomorrow_entry))\n",
        "         #print(i)\n",
        "  return today, tomorrow, today_timestamp, tomorrow_timestamp\n",
        "\n",
        "today, tomorrow, today_timestamp, tomorrow_timestamp = get_todaytomorrow(data, includeTemp=includeTemp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'birdID'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-68895a005b8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtoday\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtomorrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoday_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtomorrow_timestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtoday\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtomorrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoday_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtomorrow_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_todaytomorrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludeTemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mincludeTemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-68895a005b8a>\u001b[0m in \u001b[0;36mget_todaytomorrow\u001b[0;34m(data, includeTemp)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tavg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tavg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# only add pair if both contain basic weather data (avg temp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'birdID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'birdID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                \u001b[0;31m# only add if birdIDs match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m           \u001b[0mtoday_day\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_to_nth_day\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0mtomorrow_day\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_to_nth_day\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'birdID'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a8pE1w8WEFo"
      },
      "source": [
        "# Split the data into training and testing sets\n",
        "features = today[:,1:] #data.values[:,:-1]\n",
        "labels = tomorrow[:,1:] #[:,[1,2]] #data.values[:, 60]\n",
        "#features = features.reshape(features.shape[0], features.shape[1], 1) if using CNN, may need to use this\n",
        "import math\n",
        "# Split the data into training and testing sets cronologically\n",
        "\n",
        "todaydf = pd.DataFrame(today)\n",
        "tomorrowdf = pd.DataFrame(tomorrow)\n",
        "\n",
        "def train_test_split(datafile, tomorrow, train_ratio, includeTemp):\n",
        "    \n",
        "    # Define dataframes to return\n",
        "    if includeTemp:\n",
        "      xheaders = ['AnimalID', 'TS', 'Lat - 1', 'Long - 1', 'TAvg', 'TMin', 'TMax']\n",
        "    else:\n",
        "      xheaders = ['AnimalID', 'TS', 'Lat - 1', 'Long - 1']\n",
        "    yheaders = ['AnimalID', 'Lat', 'Long']\n",
        "    \n",
        "    trainx = pd.DataFrame(columns = xheaders)\n",
        "    trainy = pd.DataFrame(columns = yheaders)\n",
        "    testx = pd.DataFrame(columns = xheaders)\n",
        "    testy = pd.DataFrame(columns = yheaders)\n",
        "    \n",
        "    # Split each unique animal tracking info into training and testing sets\n",
        "    \n",
        "    for animal in datafile[0].unique():\n",
        "        traincount = math.ceil(float(datafile[datafile[0] == animal].shape[0]) * train_ratio)\n",
        "        obscount = datafile[datafile[0] == animal].shape[0]\n",
        "        obslist = datafile.index[datafile[0] == animal].tolist()\n",
        "        \n",
        "        for obs in range(0, traincount, 1):\n",
        "            if includeTemp:\n",
        "              dfx = [animal, datafile[1][obslist[obs]], datafile[2][obslist[obs]], datafile[3][obslist[obs]],\n",
        "                   datafile[4][obslist[obs]], datafile[5][obslist[obs]], datafile[6][obslist[obs]]]\n",
        "            else:\n",
        "              dfx = [animal, datafile[1][obslist[obs]], datafile[2][obslist[obs]], datafile[3][obslist[obs]]]\n",
        "            dfy = [animal, tomorrow[1][obslist[obs]], tomorrow[2][obslist[obs]]]\n",
        "            \n",
        "            trainx.loc[len(trainx.index)] = dfx\n",
        "            trainy.loc[len(trainy.index)] = dfy\n",
        "\n",
        "        for obs in range(traincount, obscount, 1):\n",
        "            if includeTemp:\n",
        "              dfx = [animal, datafile[1][obslist[obs]], datafile[2][obslist[obs]], datafile[3][obslist[obs]],\n",
        "                   datafile[4][obslist[obs]], datafile[5][obslist[obs]], datafile[6][obslist[obs]]]\n",
        "            else:\n",
        "              dfx = [animal, datafile[1][obslist[obs]], datafile[2][obslist[obs]], datafile[3][obslist[obs]]]\n",
        "            dfy = [animal, tomorrow[1][obslist[obs]], tomorrow[2][obslist[obs]]]\n",
        "            \n",
        "            testx.loc[len(testx.index)] = dfx\n",
        "            testy.loc[len(testy.index)] = dfy\n",
        "    \n",
        "    return(trainx, trainy, testx, testy)\n",
        "\n",
        "trainin_x, trainin_y, testin_x, testin_y = train_test_split(todaydf, tomorrowdf, 0.8,includeTemp)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-QcR9C2QhXQ"
      },
      "source": [
        "trainin_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBYtzSx84g8L"
      },
      "source": [
        "# converts data into correct type for keras model\n",
        "if includeTemp:\n",
        "  training_x = np.transpose(np.asarray([trainin_x['TS'], trainin_x['Lat - 1'], trainin_x['Long - 1'], trainin_x['TAvg'], trainin_x['TMin'], trainin_x['TMax']]).astype('float32'))\n",
        "  testing_x = np.transpose(np.asarray([testin_x['TS'], testin_x['Lat - 1'], testin_x['Long - 1'], testin_x['TAvg'], testin_x['TMin'], testin_x['TMax']]).astype('float32'))\n",
        "else:\n",
        "  training_x = np.transpose(np.asarray([trainin_x['TS'], trainin_x['Lat - 1'], trainin_x['Long - 1']]).astype('float32'))\n",
        "  testing_x = np.transpose(np.asarray([testin_x['TS'], testin_x['Lat - 1'], testin_x['Long - 1']]).astype('float32'))\n",
        "training_y = np.transpose(np.asarray([trainin_y['Lat'], trainin_y['Long']]).astype('float32'))\n",
        "testing_y = np.transpose(np.asarray([testin_y['Lat'], testin_y['Long']]).astype('float32'))\n",
        "# from https://stackoverflow.com/questions/48851558/tensorflow-estimator-valueerror-logits-and-labels-must-have-the-same-shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cPP4XbxbrSb"
      },
      "source": [
        "# Regression Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLm0DyC9mcwT"
      },
      "source": [
        "# Regression packages\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsbPqC3581Em"
      },
      "source": [
        "# Normalization \n",
        "normalizer = preprocessing.Normalization()\n",
        "normalizer.adapt(np.array(training_x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hijc_DiCOlP"
      },
      "source": [
        "from tensorflow import optimizers\n",
        "best_eval = 10000 # big number\n",
        "for i  in  range(1,10) :\n",
        "  # Build model, add layers\n",
        "  input_size = training_x.shape[1]\n",
        "  my_model = Sequential([normalizer, Dense(units=1)])\n",
        "  my_model.add(Dense(32, input_dim=input_size, kernel_initializer='normal', activation='relu'))\n",
        "  #my_model.add(Conv1D(32, 3, activation=\"relu\"))\n",
        "  # my_model.add(Dense(16,  kernel_initializer='normal', activation='relu'))\n",
        "  #my_model.add(Dense(16,  kernel_initializer='normal', activation='relu'))\n",
        "  my_model.add(Dense(8,  kernel_initializer='normal', activation='relu'))\n",
        "  my_model.add(Dense(2, kernel_initializer='normal'))\n",
        "  \n",
        "  # Train model\n",
        "  my_model.compile(optimizer=optimizers.Adam(learning_rate=0.1),loss='mean_absolute_error')\n",
        "  history = my_model.fit(training_x, training_y,epochs=50,verbose=0,validation_split = 0.2)\n",
        "\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "  hist.tail()\n",
        "\n",
        "  this_eval = my_model.evaluate(testing_x,testing_y, verbose=0) # hist.loss[49]\n",
        "  print(this_eval) \n",
        "  if (this_eval< best_eval):\n",
        "    print('this eval is better than best eval')\n",
        "    best_eval = this_eval\n",
        "    best_model = my_model\n",
        "    best_history = history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmlA3hLsCL6L"
      },
      "source": [
        "# Plot loss by Epoch to evaluate model improvement\n",
        "\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  #plt.ylim([0, 10])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "plot_loss(best_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTPpms_MDcIj"
      },
      "source": [
        "best_model.evaluate(testing_x,testing_y, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS7LTwYj1tGX"
      },
      "source": [
        "best_model.evaluate(training_x,training_y, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNi758gMEg7Q"
      },
      "source": [
        "# Make predictions \n",
        "\n",
        "train_predictions = best_model.predict(training_x)#.flatten()\n",
        "test_predictions = best_model.predict(testing_x)#.flatten()\n",
        "\n",
        "test_predictions\n",
        "fig,ax = plt.subplots()\n",
        "\n",
        "ax.plot(testing_y[:,0], testing_y[:,1], 'o', label='Actual')\n",
        "ax.plot(test_predictions[:,0], test_predictions[:,1], 'o', label='Predicted')\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IwLMsR4VKZY"
      },
      "source": [
        "testing_error = np.sqrt((test_predictions[:,0] - testing_y[:,0])**2 + (test_predictions[:,1] - testing_y[:,1])**2)\n",
        "training_error = np.sqrt((train_predictions[:,0] - training_y[:,0])**2 + (train_predictions[:,1] - training_y[:,1])**2)\n",
        "# need to change this to account for lat/long to get real distance between points\n",
        "\n",
        "print(f'Mean testing error: {np.mean(testing_error)}')\n",
        "print(f'Mean training error: {np.mean(training_error)}')\n",
        "\n",
        "#fig = plt.figure()\n",
        "#ax = plt.axes(projection='3d')\n",
        "#ax.scatter3D(training_y[:,0],training_y[:,1], training_error, label='Training Error', marker='x', depthshade=False)\n",
        "#ax.scatter3D(testing_y[:,0],testing_y[:,1], testing_error,label='Testing Error', marker='x',depthshade=False)\n",
        "#ax.legend()\n",
        "\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "bins_list = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
        "fig = plt.figure()\n",
        "plt.hist([training_error, testing_error], bins=bins_list, label=['Training Error', 'Testing Error'], weights=[np.ones(len(training_error))/len(training_error), np.ones(len(testing_error))/len(testing_error)])\n",
        "#plt.hist(training_error, weights=np.ones(len(training_error))/len(training_error), label='Training Error', bins=bins_list )\n",
        "#plt.hist(testing_error, weights=np.ones(len(testing_error))/len(testing_error), label='Testing Error',bins=bins_list)\n",
        "\n",
        "plt.xlabel('Error')\n",
        "plt.ylabel('Percentage of Prediction')\n",
        "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
        "plt.xlim(0,15)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoRCNQdCFmF0"
      },
      "source": [
        "# Plot prediction on map \n",
        "\n",
        "from shapely.geometry import Point, LineString, MultiPoint\n",
        "import geopandas as gpd\n",
        "from geopandas import GeoDataFrame\n",
        "\n",
        "actual_plot=gpd.GeoDataFrame([[MultiPoint(np.flip(testing_y, axis=1))]],columns=['geometry'])\n",
        "predicted_plot=gpd.GeoDataFrame([[MultiPoint(np.flip(test_predictions, axis=1))]],columns=['geometry'])\n",
        "training_plot = gpd.GeoDataFrame([[MultiPoint(np.flip(training_y, axis=1))]], columns=['geometry'])\n",
        "minx, miny, maxx, maxy = actual_plot.geometry.total_bounds\n",
        " \n",
        "fig, ax = plt.subplots(figsize=(20,12))\n",
        "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "\n",
        "world.plot(ax=ax)\n",
        "training_plot.plot(ax=ax,marker='o',label='Training', color='cyan', markersize=15)\n",
        "actual_plot.plot(ax=ax, marker='o', label='Actual', color='orange')\n",
        "predicted_plot.plot(ax=ax,marker='o',label='Predicted', color='red')\n",
        "\n",
        "ax.set_xlim(minx - 15, maxx + 15) # added/substracted value is to give some margin around total bounds\n",
        "ax.set_ylim(miny - 15, maxy + 15)\n",
        "ax.legend()\n",
        "#https://gis.stackexchange.com/questions/332624/geopandas-plot-two-layers-but-only-to-the-extent-of-the-smaller-one"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir1uL6vtSFE4"
      },
      "source": [
        "# Predicting consecutive days of migration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ11rG2Syz3m"
      },
      "source": [
        "def getWeather(stations, lat, long, mig_datetime):\n",
        "   #stations = Stations()\n",
        "   stations = stations.nearby(lat = 33.52068, lon = -86.81176,radius=40000) #note: radius is in meters (40k meters ~ 25 miles)\n",
        "   station = stations.fetch(1)\n",
        "   weather_data = Daily(station, start=mig_datetime,end=(mig_datetime+relativedelta(days=0)))\n",
        "   weather_data = weather_data.fetch()\n",
        "   \n",
        "   weather_data_temp = weather_data[['tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wdir', 'wspd', 'wpgt', 'pres', 'tsun']].values.tolist()\n",
        "   if (not weather_data_temp): # sometimes it just creates an empty list. This checks/fixes it \n",
        "     weather_data_temp = np.empty((1,10), dtype='float')\n",
        "     weather_data_temp[:] = np.NaN\n",
        "   for i in range(2, stations.count()-1):\n",
        "     if (np.isnan(weather_data_temp).any):\n",
        "      station = stations.fetch(i)\n",
        "      weather_data = Daily(station, start=mig_datetime,end=(mig_datetime + relativedelta(days=1)))\n",
        "      weather_data = weather_data.fetch()\n",
        "\n",
        "      #a = weather_data['wdir'].values[0].tolist()\n",
        "      #print(a)\n",
        "      try:\n",
        "\n",
        "        if np.isnan(weather_data_temp[0][0]): #tavg\n",
        "          [weather_data_temp[0][0]] = weather_data['tavg'].values.tolist()\n",
        "        if np.isnan(weather_data_temp[0][1]): #tmin tmax\n",
        "          [weather_data_temp[0][1]] = weather_data['tmin'].values.tolist()\n",
        "          [weather_data_temp[0][2]] = weather_data['tmax'].values.tolist()\n",
        "        if np.isnan(weather_data_temp[0][3]): #prcp\n",
        "          [weather_data_temp[0][3]] = weather_data['prcp'].values.tolist()\n",
        "        if np.isnan(weather_data_temp[0][4]): #snow\n",
        "          [weather_data_temp[0][4]] = weather_data['snow'].values.tolist()\n",
        "        if np.isnan(weather_data_temp[0][5]): #wdir\n",
        "          [weather_data_temp[0][5]] = weather_data['wdir'].values.tolist()\n",
        "        if np.isnan(weather_data_temp[0][6]): #wspd\n",
        "          [weather_data_temp[0][6]] = weather_data['wspd'].values.tolist()\n",
        "        if np.isnan(weather_data_temp[0][7]): #wpgt\n",
        "          [weather_data_temp[0][7]] = weather_data['wpgt'].values.tolist()\n",
        "        if np.isnan(weather_data_temp[0][8]): #pres\n",
        "          [weather_data_temp[0][8]] = weather_data['pres'].values.tolist()\n",
        "        if np.isnan(weather_data_temp[0][9]): #tsun\n",
        "          [weather_data_temp[0][9]] = weather_data['tsun'].values.tolist()\n",
        "      except:\n",
        "        print(\"exception: \", weather_data)\n",
        "   station_data = station.values[0]\n",
        "   return weather_data_temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujqyBbt8ydOR"
      },
      "source": [
        "#getWeather(stations, longtest_x[rows-1][0], longtest_x[rows-1][1], dt.datetime.strptime(tomorrow_timestamp[:][rows-1], \"%Y-%m-%d %H:%M:%S\"))\n",
        "#getWeather(stations, longtest_x[rows][0], longtest_x[rows][1], dt.datetime.strptime(tomorrow_timestamp[:][rows], \"%Y-%m-%d %H:%M:%S\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFmePQQrST6Z"
      },
      "source": [
        " from dateutil.relativedelta import relativedelta\n",
        "\n",
        "longtest_x = testing_x\n",
        "longtest_y = best_model.predict([longtest_x[:][0].tolist()])\n",
        "\n",
        "for rows in range(1, len(testin_x), 1):\n",
        "  if includeTemp:\n",
        "    stations = Stations()\n",
        "    weather_data = getWeather(stations, longtest_x[rows][0], longtest_x[rows][1], dt.datetime.strptime(tomorrow_timestamp[:][rows], \"%Y-%m-%d %H:%M:%S\"))\n",
        "    longtest_x_row = [longtest_x[:][rows].tolist()]\n",
        "  else:\n",
        "    longtest_x_row = [longtest_x[:][rows].tolist()]\n",
        "\n",
        "  predict = best_model.predict(longtest_x_row)\n",
        "  longtest_y = np.vstack((longtest_y, predict))\n",
        "\n",
        "  if len(testin_x) > rows + 1:\n",
        "    if testin_x['AnimalID'][rows] == testin_x['AnimalID'][rows + 1]:\n",
        "      longtest_x[rows + 1, 1] = predict[0][0]\n",
        "      longtest_x[rows + 1, 2] = predict[0][1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfX4ol1YbpHK"
      },
      "source": [
        "best_model.evaluate(testing_x, testing_y, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfHgLC9KrikH"
      },
      "source": [
        "# Plot updated prediction on map \n",
        "actual_plot=gpd.GeoDataFrame([[MultiPoint(np.flip(testing_y, axis=1))]],columns=['geometry'])\n",
        "predicted_plot=gpd.GeoDataFrame([[MultiPoint(np.flip(longtest_y, axis=1))]],columns=['geometry'])\n",
        "training_plot = gpd.GeoDataFrame([[MultiPoint(np.flip(training_y, axis=1))]], columns=['geometry'])\n",
        "minx, miny, maxx, maxy = actual_plot.geometry.total_bounds\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20,12))\n",
        "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "\n",
        "world.plot(ax=ax)\n",
        "training_plot.plot(ax=ax,marker='o',label='Training', color='cyan', markersize=15)\n",
        "actual_plot.plot(ax=ax, marker='o', label='Actual', color='orange')\n",
        "predicted_plot.plot(ax=ax,marker='o',label='Predicted', color='red')\n",
        "\n",
        "ax.set_xlim(minx - 5, maxx + 5) # added/substracted value is to give some margin around total bounds\n",
        "ax.set_ylim(miny - 5, maxy + 5)\n",
        "ax.legend()\n",
        "#https://gis.stackexchange.com/questions/332624/geopandas-plot-two-layers-but-only-to-the-extent-of-the-smaller-one"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FImkX-xrxkkG"
      },
      "source": [
        "\n",
        "np.savetxt(\"goose_predict_temp.csv\", longtest_y, delimiter=\",\")\n",
        "np.savetxt(\"goose_actual_temp.csv\", testing_y,  delimiter=\",\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZquWwsuG5H2"
      },
      "source": [
        "# Test model if global warming causes 2 degree warming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnVc_MA0G4Vq"
      },
      "source": [
        "if includeTemp: \n",
        "  testing_x_gw = testing_x\n",
        "  testing_x_gw[:][4:6] = testing_x[:][4:6]+2\n",
        "  test_predictions_gw = best_model.predict(testing_x)#.flatten()\n",
        "\n",
        "  fig,ax = plt.subplots()\n",
        "\n",
        "  ax.plot(test_predictions[:,0], test_predictions[:,1], 'o', label='Predicted w/o global warming')\n",
        "  ax.plot(test_predictions_gw[:,0], test_predictions_gw[:,1], 'o', label='Predicted w/ global warming (+2C)')\n",
        "  ax.legend()\n",
        "\n",
        "  change_with_gw = np.sqrt((test_predictions[:,0] - test_predictions_gw[:,0])**2 + (test_predictions[:,1] - test_predictions_gw[:,1])**2)\n",
        "  print(np.mean(change_with_gw))\n",
        "  \n",
        "  fig,ax1 = plt.subplots()\n",
        "\n",
        "  ax1.plot(test_predictions[:,0], label='latitude w/o global warming')\n",
        "  ax1.plot(test_predictions[:,1], label='longitude w/o global warming')\n",
        "  ax1.plot(test_predictions_gw[:,0], label='latitude w/ global warming (+2C)')\n",
        "  ax1.plot(test_predictions_gw[:,1], label='longitude w/ global warming (+2C)')\n",
        "  #ax1.plot(test_predictions_gw[:,0], test_predictions_gw[:,1], 'o', label='Predicted w/ global warming (+2C)')\n",
        "  ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxKxJ9N3YrrT"
      },
      "source": [
        "if includeTemp:\n",
        "  np.savetxt(\"goose_nogw.csv\", test_predictions, delimiter=\",\")\n",
        "  np.savetxt(\"goose_gw.csv\", test_predictions_gw,  delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}